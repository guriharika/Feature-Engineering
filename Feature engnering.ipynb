{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "t8-B5K6tksSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment Questions**"
      ],
      "metadata": {
        "id": "FMo8xoUMksQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.What is a parameter?**"
      ],
      "metadata": {
        "id": "HDb_jPJ1ksOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:A parameter is a variable or value that is used to define or control a function, process, or system. It acts as an input or setting that influences how something behaves or operates.\n",
        "\n",
        "**Here are a few examples in different contexts:**\n",
        "\n",
        "1.**In programming**: A parameter is a value that is passed to a function when it is called. It allows the function to use that value in its calculations or actions.\n",
        "\n",
        "2.**In mathematics:** A parameter can be a constant in a mathematical equation or a set of values that are used to describe a function or system. For instance, in the equation of a line, y = mx + b, the variables m (slope) and b (y-intercept) are parameters that determine the line's behavior.\n",
        "\n",
        "3.**In statistics:**A parameter refers to a characteristic or measure of a population, such as the population mean or standard deviation.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MEn4Y2saksL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.What is correlation?**\n",
        "\n",
        "**What does negative correlation mean?**"
      ],
      "metadata": {
        "id": "rgqLEy1oksJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Correlation refers to a statistical relationship or connection between two or more variables, where changes in one variable are associated with changes in another. It measures how strongly two variables are related and the direction of their relationship.\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases, and vice versa. It is represented by a correlation coefficient between -1 and 0. The closer the value is to -1, the stronger the negative relationship.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GIiSkR4dksHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.Define Machine Learning. What are the main components in Machine Learning?**"
      ],
      "metadata": {
        "id": "POk7rCZVksEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Machine Learning is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to allow computers to perform specific tasks without explicit programming. Instead of being directly programmed to perform tasks, ML systems learn from data and improve over time based on patterns and insights they discover.\n",
        "\n",
        "In simpler terms, Machine Learning enables systems to automatically learn and make predictions or decisions based on historical data, without human intervention in programming the logic for each task.\n",
        "\n",
        "**Main Components in Machine Learning:**\n",
        "\n",
        "**Data:**\n",
        "\n",
        "Data is the foundation of ML. It consists of raw facts, numbers, and information that the algorithm will learn from. The data can be structured (like spreadsheets or databases) or unstructured (like text, images, and videos).\n",
        "Training Data: The data used to teach the model how to make predictions.\n",
        "Test Data: The data used to evaluate how well the model has learned from the training data and to check for overfitting.\n",
        "\n",
        "**Model:**\n",
        "\n",
        "A model in ML represents a mathematical structure that defines how input data will be processed to make predictions or decisions. Models are built using algorithms and then trained on data.\n",
        "Examples of models include decision trees, linear regression, neural networks, and support vector machines.\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "An algorithm is a set of rules or steps that defines how a machine learning model is built. It is the process by which a machine learning model learns from the training data.\n",
        "Examples of algorithms include:\n",
        "Supervised Learning Algorithms: Linear Regression, Decision Trees, etc.\n",
        "Unsupervised Learning Algorithms: K-Means, DBSCAN, etc.\n",
        "Reinforcement Learning Algorithms: Q-Learning, Deep Q-Networks (DQN), etc.\n",
        "\n",
        "**Training:**\n",
        "\n",
        "Training is the process where the model is exposed to data and learns the relationships between input and output. This phase involves adjusting the model's parameters to minimize errors or maximize accuracy (depending on the type of ML task).\n",
        "Loss function or cost function is used to measure how well the model is performing. The goal is to minimize this loss during training.\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "After training, the model is tested using separate data (called test data) to see how well it generalizes to new, unseen examples.\n",
        "Common evaluation metrics for ML models include accuracy, precision, recall, F1-score, and mean squared error (MSE), depending on the type of problem (classification, regression, etc.).\n",
        "\n",
        "**Prediction/Inference:**\n",
        "\n",
        "Prediction is the process where the trained model is used to make predictions or decisions based on new data. This is the phase where the model applies its learned knowledge to real-world data.\n",
        "\n",
        "**Feature Engineering:**\n",
        "\n",
        "Feature engineering refers to the process of selecting, modifying, or creating features (input variables) from raw data to improve the model's performance.\n",
        "Good features can significantly improve the accuracy and efficiency of machine learning models.\n",
        "\n",
        "**Hyperparameters:**\n",
        "\n",
        "These are parameters that control the learning process but are set before training (such as learning rate, number of layers in a neural network, etc.).\n",
        "Tuning hyperparameters through processes like grid search or random search can significantly impact model performance.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "99qs66J3ksBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.How does loss value help in determining whether the model is good or not?**"
      ],
      "metadata": {
        "id": "dOpL3RjQkr_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:The loss value quantifies how well a machine learning model is performing by measuring the difference between predicted and actual values. During training, the model aims to minimize this loss to improve its accuracy.\n",
        "\n",
        "* Smaller Loss: Indicates better performance, as predictions are closer to the true values.\n",
        "\n",
        "* Larger Loss: Indicates poor performance, with predictions farther from the actual values.\n",
        "\n",
        "Loss helps in detecting:\n",
        "\n",
        "* Overfitting: When the model performs well on training data but poorly on test data (high test loss).\n",
        "\n",
        "* Underfitting: When the model performs poorly on both training and test data.\n",
        "By minimizing the loss value, the model improves its predictions, making it a key measure of how good the model is.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SWAJJ7vzkr8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "lP6Sh-pXkr5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:**Continuous Variables:**\n",
        "\n",
        "Definition: These are variables that can take any value within a certain range. They are numeric and can represent quantities that are measured on a continuous scale.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Can take any value (integers or decimals).\n",
        "\n",
        "* There is an infinite number of possible values between any two values.\n",
        "\n",
        "* Common examples include height, weight, temperature, age, salary, etc.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "A person's height (can be 170.2 cm, 170.23 cm, or 170.235 cm) is a continuous variable because it can take any value within a given range.\n",
        "\n",
        "**Categorical Variables:**\n",
        "\n",
        "Definition: These are variables that represent categories or groups. The values of categorical variables are distinct, discrete labels.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Limited to a specific set of categories or values.\n",
        "\n",
        "* Can be nominal (no meaningful order) or ordinal (with a meaningful order).\n",
        "\n",
        "**Types:**\n",
        "\n",
        "Nominal: Categories with no specific order. For example, gender, color, country, etc.\n",
        "\n",
        "Ordinal: Categories with a specific order or ranking. For example, education level (high school, bachelor's, master's) or rating scales (1-5 stars).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "A person's gender (male, female, non-binary) is a categorical variable because it consists of specific, non-numeric categories.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BASPqE0Bkr2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?**"
      ],
      "metadata": {
        "id": "vnKm1jqXkrz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Handling categorical variables in machine learning is essential because most algorithms expect numerical input. There are several techniques to convert categorical data into a format that machine learning algorithms can process. Here are some of the most common methods:\n",
        "\n",
        "**1. Label Encoding:**\n",
        "\n",
        "**What it is:** Converts each category in a categorical variable into a unique integer. Each unique category is assigned an integer value (e.g., 0, 1, 2, 3).\n",
        "\n",
        "**When to use:** Works well when the categorical variable has an ordinal relationship (where the categories have a natural order, like \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "**Example:** For the variable \"Size\" with values [Small, Medium, Large], label encoding might convert them as:\n",
        "\n",
        "* Small -> 0\n",
        "* Medium -> 1\n",
        "* Large -> 2\n",
        "\n",
        "**2. One-Hot Encoding:**\n",
        "\n",
        "**What it is:** Creates binary columns (0 or 1) for each category in the variable. For each instance, only one of the new columns will be 1, and the rest will be 0. This method does not introduce any ordering among the categories.\n",
        "\n",
        "**When to use:** Ideal for nominal categorical variables (no inherent order, such as colors, types, etc.).\n",
        "\n",
        "**Example:** For the variable \"Color\" with values [Red, Blue, Green], one-hot encoding would create three new binary columns\n",
        "\n",
        "**3. Binary Encoding:**\n",
        "\n",
        "**What it is:** A compromise between Label Encoding and One-Hot Encoding. Each category is first encoded as an integer, and then the integer is converted to binary.\n",
        "\n",
        "**When to use:** Useful when there are many categories and one-hot encoding would result in too many new features.\n",
        "\n",
        "**Example:** For three categories [Red, Blue, Green], label encode them first:\n",
        "\n",
        "* Red -> 0\n",
        "* Blue -> 1\n",
        "* Green -> 2 Then, convert these numbers to binary:\n",
        "* 0 -> 00\n",
        "* 1 -> 01\n",
        "* 2 -> 10\n",
        "\n",
        "**4. Target (Mean) Encoding:**\n",
        "\n",
        "**What it is:** Replaces each category with the mean of the target variable for that category. This is useful for predictive tasks where the encoding can directly capture relationships between the feature and the target.\n",
        "\n",
        "**When to use:** Typically used when the categorical variable has a significant impact on the target variable.\n",
        "\n",
        "**Example:** For a categorical feature \"City\" and a target \"Price,\" the encoding would replace each city with the average price for that city.\n",
        "\n",
        "* New York -> Mean(Price for New York)\n",
        "\n",
        "* London -> Mean(Price for London)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XmzB3xcJkrw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.What do you mean by training and testing a dataset?**"
      ],
      "metadata": {
        "id": "EzD8wsO7krt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Training and testing a dataset are terms commonly used in machine learning and data science to describe the process of using a dataset to train and evaluate a model. Here’s what they mean:\n",
        "\n",
        "**1. Training a Dataset:**\n",
        "\n",
        "* Training refers to the process of feeding a machine learning algorithm a dataset so that it can learn patterns, relationships, or structures in the data.\n",
        "\n",
        "* During training, the model is exposed to input data (features) along with the corresponding correct outputs (labels) in a supervised learning context.\n",
        "\n",
        "* The model adjusts its internal parameters (weights) in an attempt to minimize the error between its predictions and the actual labels.This is often done through optimization techniques like gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LApzgPookrrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "jBRzew33kroV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:sklearn.preprocessing is a module in scikit-learn (a popular Python machine learning library) that provides a variety of utilities for preprocessing data. The preprocessing step is important in machine learning because it involves transforming raw data into a format that is suitable for training machine learning models. This can involve scaling features, encoding categorical variables, and handling missing values, among other things.\n",
        "\n",
        "**Some common tools in sklearn.preprocessing include:**\n",
        "\n",
        "**Scaling and Normalizing Features:**\n",
        "\n",
        "**StandardScaler:**\n",
        "\n",
        "* Standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "* MinMaxScaler: Scales features to a given range, typically [0, 1].\n",
        "\n",
        "* RobustScaler: Scales features using statistics that are robust to outliers (e.g., using the median and interquartile range).\n",
        "\n",
        "* Normalizer: Scales individual samples to have unit norm (i.e., normalizes each row of data).\n",
        "\n",
        "**Encoding Categorical Variables:**\n",
        "\n",
        "* LabelEncoder: Converts categorical labels into numeric form (used for target labels).\n",
        "\n",
        "* OneHotEncoder: Converts categorical features into a one-hot encoded format, where each category is represented as a binary vector.\n",
        "\n",
        "**Imputing Missing Values:**\n",
        "\n",
        "* SimpleImputer: Replaces missing values (NaNs) with a specified strategy (mean, median, or a constant value).\n",
        "\n",
        "**Binarization:**\n",
        "\n",
        "* Binarizer: Transforms features into binary values based on a threshold. This can be useful when you need to convert numeric features into 0 or 1 values.\n",
        "\n",
        "**Polynomial Features:**\n",
        "\n",
        "* PolynomialFeatures: Generates polynomial and interaction features, which can be useful for fitting non-linear models.\n",
        "\n",
        "**Power Transformation:**\n",
        "\n",
        "* PowerTransformer: Applies power transformations to make data more Gaussian-like, which can help improve the performance of certain models.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "abp3LaZ8krlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.What is a Test set?**"
      ],
      "metadata": {
        "id": "V1NvNK25krit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:A test set is a subset of a dataset used to evaluate the performance of a machine learning model after it has been trained. It plays a crucial role in assessing how well the model generalizes to new, unseen data, which is important because you want your model to perform well not just on the data it was trained on, but also on data it has never encountered before.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wvaAkjuhkrf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "**How do you approach a Machine Learning problem?**"
      ],
      "metadata": {
        "id": "vzpM_2fokrdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:In Python, the most common way to split data for model fitting (training and testing) is by using the train_test_split() function from scikit-learn. This function randomly splits a dataset into a training set and a testing set.\n",
        "\n"
      ],
      "metadata": {
        "id": "60WfV8aFkraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# X_train and y_train are used for training\n",
        "# X_test and y_test are used for testing the model\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_BMjEvZrKfy",
        "outputId": "e9d1bebd-d7e2-4e77-d688-7b07f3305719"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (120, 4)\n",
            "Test set shape: (30, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Approach a Machine Learning Problem?\n",
        "Approaching a machine learning problem involves several steps. Here’s a structured process that can guide you:\n",
        "\n",
        "**1. Define the Problem**\n",
        "\n",
        "* Objective: Understand the goal of the problem. What do you want to predict or classify? This will determine if you're working on a regression, classification, or clustering problem.\n",
        "\n",
        "* Business Context: Know the application or real-world context to ensure that the problem-solving is meaningful.\n",
        "\n",
        "**2. Collect and Understand the Data**\n",
        "\n",
        "* Data Collection: Gather the data needed for training and testing your model. This can come from various sources such as databases, files (CSV, Excel), or APIs.\n",
        "\n",
        "* Exploratory Data Analysis (EDA): Analyze the data to understand its structure, identify relationships, and spot potential issues such as missing values or outliers.\n",
        "\n",
        "**3. Data Preprocessing**\n",
        "\n",
        "* Cleaning Data: Handle missing values, duplicates, and erroneous data.\n",
        "Feature Engineering: Create or transform new features based on the existing ones (e.g., combining columns or extracting meaningful values from raw data).\n",
        "\n",
        "* Encoding Categorical Features: Convert categorical features into numerical values using techniques like OneHotEncoding or LabelEncoding.\n",
        "\n",
        "* Scaling Features: Normalize or standardize features to bring them to a common scale (e.g., using StandardScaler or MinMaxScaler).\n",
        "\n",
        "**4. Split the Data**\n",
        "\n",
        "* Use train-test split (or cross-validation) to divide your dataset into training and testing (or validation) sets. This ensures that the model’s performance is tested on data it hasn’t seen before.\n",
        "\n",
        "**5. Choose a Model**\n",
        "\n",
        "* Select an appropriate machine learning algorithm. The choice depends on the problem type (classification, regression, etc.), the size of the dataset, and the complexity of the task.\n",
        "\n",
        "* For instance:\n",
        "\n",
        "* Classification: Decision Trees, Random Forest, Logistic Regression, SVM, k-NN, etc.\n",
        "\n",
        "* Regression: Linear Regression, Decision Trees, Random Forest, etc.\n",
        "\n",
        "* Clustering: k-Means, DBSCAN, Hierarchical Clustering.\n",
        "\n",
        "**6. Train the Model**\n",
        "\n",
        "Fit the selected model to the training data using the fit() function.\n",
        "\n",
        "**7. Evaluate the Model**\n",
        "\n",
        "* After training the model, evaluate its performance using the test set.\n",
        "For classification problems, you might use metrics like accuracy, precision, recall, F1-score, or confusion matrix.\n",
        "\n",
        "* For regression problems, metrics like Mean Squared Error (MSE), R-squared, or Mean Absolute Error (MAE) are commonly used.\n",
        "\n",
        "**8. Hyperparameter Tuning**\n",
        "\n",
        "* After the initial evaluation, you can improve the model by tuning its hyperparameters (e.g., using GridSearchCV or RandomizedSearchCV).\n",
        "\n",
        "* This process involves testing different combinations of hyperparameters to find the best-performing ones.\n",
        "\n",
        "**9. Model Validation**\n",
        "\n",
        "* Cross-validation or additional validation strategies can be used to verify the performance of the model, ensuring that it’s not overfitting to the training data.\n",
        "\n",
        "**10. Model Deployment**\n",
        "\n",
        "* Once satisfied with the model’s performance, deploy the model into production or integrate it into a real-world application for making predictions on new data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e_KmTA4rkrVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.Why do we have to perform EDA before fitting a model to the data?**"
      ],
      "metadata": {
        "id": "A89xevGnkrSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:EDA (Exploratory Data Analysis) is essential before fitting a model because it provides a deep understanding of the dataset. Through EDA, you can:\n",
        "\n",
        "**1.Identify missing or incorrect data:**This allows you to clean and preprocess the data appropriately.\n",
        "\n",
        "**2.Detect outliers:**Outliers can skew results and affect model accuracy, so you need to understand them first.\n",
        "\n",
        "**3.Understand distributions:** Knowing the distribution of features helps in selecting the right modeling techniques (e.g., normal vs. non-normal data).\n",
        "\n",
        "**4.Examine correlations:** EDA allows you to see relationships between variables, helping to identify which ones might be important for the model.\n",
        "\n",
        "**5.Ensure assumptions:** Many models assume certain characteristics (like linearity or homoscedasticity). EDA helps check if these assumptions hold.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vwj9VRt0krPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.What is correlation?**"
      ],
      "metadata": {
        "id": "uH3QbT_BkrMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It shows how changes in one variable are associated with changes in another.\n",
        "\n",
        "* Positive correlation: When one variable increases, the other also tends to increase (e.g., height and weight).\n",
        "\n",
        "* Negative correlation: When one variable increases, the other tends to decrease (e.g., amount of exercise and weight gain).\n",
        "\n",
        "* No correlation: There's no predictable relationship between the variables (e.g., shoe size and IQ).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ylm37V-krJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.What does negative correlation mean?**"
      ],
      "metadata": {
        "id": "UwWmzLIWkrHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n",
        "For example:\n",
        "\n",
        "* Exercise and weight gain: Generally, as the amount of exercise increases, weight tends to decrease (a negative correlation).\n",
        "\n",
        "* Temperature and the need for heating: As the temperature increases, the need for heating typically decreases (also a negative correlation).\n",
        "\n",
        "The correlation coefficient for a negative correlation will be between 0 and -1. The closer it is to -1, the stronger the negative relationship.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p5py2n5fkrEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.How can you find correlation between variables in Python?**"
      ],
      "metadata": {
        "id": "auo42H6IkrBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:1. Using Pandas:\n",
        "First, you need to import the necessary libraries and load your data into a pandas DataFrame. After that, you can use the .corr() method to compute the correlation between variables.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "VxLzDAnmkq-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example: Create a DataFrame\n",
        "data = {\n",
        "    'X': [1, 2, 3, 4, 5],\n",
        "    'Y': [5, 4, 3, 2, 1],\n",
        "    'Z': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate Pearson correlation (default)\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgXcfN3ByOXw",
        "outputId": "33d88d3c-b97f-4b10-f7fe-4951b248e484"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     X    Y    Z\n",
            "X  1.0 -1.0  1.0\n",
            "Y -1.0  1.0 -1.0\n",
            "Z  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "* df.corr() computes the Pearson correlation coefficient by default.\n",
        "\n",
        "* The values range from -1 (perfect negative correlation) to 1 (perfect positive correlation). A value of 0 indicates no correlation.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CHOHAodRkq8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.What is causation? Explain difference between correlation and causation with an example**"
      ],
      "metadata": {
        "id": "1_VKP9ukkq4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Causation refers to the relationship between two events where one event (the cause) directly influences the occurrence of the other event (the effect). In a causal relationship, the cause is responsible for bringing about the effect.\n",
        "\n",
        "Correlation, on the other hand, refers to a statistical relationship between two variables, where they tend to change in a similar or opposite pattern. However, correlation does not imply causation, meaning that just because two things are related or change together doesn't necessarily mean one causes the other.\n",
        "\n",
        "**Example to Illustrate the Difference:**\n",
        "\n",
        "\n",
        "**Causation:**\n",
        "\n",
        "Cause: Smoking\n",
        "\n",
        "Effect: Lung cancer\n",
        "\n",
        "Smoking causes an increased risk of lung cancer. There’s scientific evidence showing that smoking leads to the development of cancer due to the harmful chemicals in cigarettes that damage lung tissue over time.\n",
        "\n",
        "**Correlation (but not causation):**\n",
        "\n",
        "Observation: Ice cream sales increase in summer.\n",
        "\n",
        "Observation: Drownings increase in summer.\n",
        "\n",
        "There’s a correlation between ice cream sales and drownings because both tend to increase in the summer months. However, eating ice cream does not cause drownings. Instead, the common factor is the warm weather, which leads to more people swimming (and sometimes drowning) and more people buying ice cream.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MPvhvkWekq0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**"
      ],
      "metadata": {
        "id": "eyfQ3riAkqxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:An optimizer in machine learning is an algorithm used to adjust the weights of a model during training to minimize the loss function. The goal is to find the best parameters (weights) for the model, making it perform as accurately as possible on new data.\n",
        "\n",
        "**Types of Optimizers:**\n",
        "\n",
        "**Gradient Descent (GD):**\n",
        "\n",
        "Description: Updates the model's parameters by computing the gradient of the loss function with respect to the parameters.\n",
        "Example: In linear regression, it adjusts weights step-by-step based on the gradient of the cost function (MSE).\n",
        "\n",
        "**Stochastic Gradient Descent (SGD):**\n",
        "\n",
        "Description: A variant of gradient descent that updates the parameters after each data point (stochastic means random).\n",
        "Example: For each data point in a dataset, SGD updates the weights incrementally, which can speed up convergence.\n",
        "\n",
        "**Mini-batch Gradient Descent:**\n",
        "\n",
        "Description: Combines batch GD and SGD by using small random subsets (mini-batches) of the dataset to update weights.\n",
        "Example: In neural networks, mini-batch sizes like 32 or 64 are commonly used for faster and more stable convergence.\n",
        "\n",
        "**Momentum:**\n",
        "\n",
        "Description: Accelerates gradient descent by adding a fraction of the previous weight update to the current one.\n",
        "Example: In SGD with momentum, it helps avoid getting stuck in local minima by adding momentum from previous steps, speeding up convergence.\n",
        "\n",
        "**Adam (Adaptive Moment Estimation):**\n",
        "\n",
        "Description: Combines the benefits of both momentum and RMSProp (adaptive learning rates).\n",
        "Example: Commonly used in deep learning, Adam adjusts the learning rate for each parameter based on both first-order (momentum) and second-order moments (scaled gradients).\n",
        "\n",
        "**RMSprop:**\n",
        "\n",
        "Description: Divides the learning rate by a moving average of the recent gradient magnitudes to stabilize the update.\n",
        "\n",
        "Example: Often used in RNNs for better performance on sequences of data, such as time series prediction.\n",
        "\n",
        "\n",
        "**Each optimizer has its strengths and is chosen based on the problem's complexity and the type of model being trained.**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8xCxMfYvkqul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.What is sklearn.linear_model ?**"
      ],
      "metadata": {
        "id": "hjmGq-5Gkqse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:sklearn.linear_model is a module in scikit-learn that provides a variety of linear models for both regression and classification tasks. These models are commonly used to predict a target variable based on linear relationships with input features. Some of the key models include:\n",
        "\n",
        "**LinearRegression:**\n",
        "\n",
        "* Use: For regression tasks, where the goal is to predict a continuous value (e.g., predicting house prices based on features like square footage and number of bedrooms).\n",
        "* Example: Predicting a target variable with a linear relationship to input features.\n",
        "\n",
        "**LogisticRegression:**\n",
        "\n",
        "* Use: For classification tasks, particularly binary or multi-class classification. It predicts the probability of a certain class.\n",
        "* Example: Predicting whether an email is spam or not (binary classification).\n",
        "\n",
        "**Ridge:**\n",
        "\n",
        "* Use: A variation of linear regression that includes L2 regularization, which helps reduce overfitting by penalizing large coefficients.\n",
        "* Example: Used when there is multicollinearity or many correlated features in the dataset.\n",
        "\n",
        "**Lasso:**\n",
        "\n",
        "* Use: A linear regression model that uses L1 regularization, which encourages sparsity and can drive some coefficients to zero, effectively performing feature selection.\n",
        "* Example: Useful when you want to reduce the number of features in your model.\n",
        "\n",
        "**ElasticNet:**\n",
        "\n",
        "* Use: Combines L1 and L2 regularization from both Lasso and Ridge regression. It is used when you want a balance between feature selection (Lasso) and model complexity control (Ridge).\n",
        "* Example: When you want the benefits of both Lasso and Ridge regularization in one model.\n",
        "\n",
        "**SGDClassifier / SGDRegressor:**\n",
        "\n",
        "* Use: These models use Stochastic Gradient Descent (SGD) for optimization, making them faster for large datasets or when you need more flexibility in model training. They can handle both classification and regression tasks.\n",
        "* Example: For large-scale linear classification or regression problems where efficiency is important.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K7brFPExkqqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.What does model.fit() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "zrLJyIcdkqnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:The model.fit() function in machine learning is used to train a model on a given dataset. It adjusts the model's parameters (like weights in a regression or neural network model) so that it can make predictions on unseen data.\n",
        "\n",
        "**What model.fit() does:**\n",
        "\n",
        "* Training the Model: It learns from the data by finding the best parameters (e.g., coefficients, weights) that minimize a loss function. This process is typically done using optimization algorithms like Gradient Descent.\n",
        "\n",
        "* Fitting the Model: It makes the model \"fit\" the data, meaning it finds the patterns or relationships between the input features and the target variable.\n",
        "\n",
        "**Arguments to model.fit():**\n",
        "\n",
        "1.X:\n",
        "\n",
        "* Description: The input data (features), usually in the form of a 2D array or DataFrame with shape (n_samples, n_features).\n",
        "* Example: In a linear regression, X could be the matrix of features like the size of houses, number of rooms, etc.\n",
        "\n",
        "2.y:\n",
        "\n",
        "* Description: The target labels (values) the model is trying to predict, usually in the form of a 1D array or vector with shape (n_samples,).\n",
        "* Example: In a linear regression model for predicting house prices, y would be the actual prices.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ePeZHWXKkqlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.What does model.predict() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "lCuetu1fkqi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:\n",
        "\n",
        "* Makes Predictions: It takes new input data (features) and predicts the target values based on the patterns the model learned during training.\n",
        "\n",
        "* Uses Learned Parameters: The model applies the parameters it learned during the training phase to the new data to predict outcomes.\n",
        "\n",
        "**Arguments to model.predict():**\n",
        "\n",
        "X:\n",
        "\n",
        "* Description: The input data (features) for which the model will make predictions. This should have the same number of features as the data used to train the model.\n",
        "\n",
        "* Format: Typically a 2D array or DataFrame with shape (n_samples, n_features), where n_samples is the number of data points to predict, and n_features is the number of features in each data point.\n",
        "\n",
        "* Example: If you're predicting house prices, X would be new data (e.g., size, number of rooms) of the houses you want to predict the price for.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sisc8UZ3kqgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "9EUVhjxckqeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:1.\n",
        "\n",
        " **Continuous Variables:**\n",
        "\n",
        "**Definition: **Continuous variables are numerical variables that can take any value within a given range. They represent measurable quantities and can have an infinite number of possible values, including fractions or decimals.\n",
        "\n",
        "**Examples:**\n",
        "* Height (e.g., 5.5 ft, 5.75 ft, 5.123 ft)\n",
        "* Weight (e.g., 60.2 kg, 72.5 kg, 88.11 kg)\n",
        "* Temperature (e.g., 36.6°C, 20.3°C)\n",
        "* Time (e.g., 1.2 seconds, 3.44 seconds)\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Can take any real number within a range.\n",
        "* Represent quantitative data (measurable).\n",
        "* Often represented as float values.\n",
        "\n",
        "**2. Categorical Variables:**\n",
        "\n",
        "Definition: Categorical variables are variables that represent categories or groups. They take on a limited, fixed number of values (called \"categories\" or \"levels\"), and these values typically don't have an inherent order or meaningful distance between them.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Nominal (no order):\n",
        "* Color (e.g., red, blue, green)\n",
        "* Gender (e.g., male, female, other)\n",
        "* Country (e.g., USA, India, UK)\n",
        "* Ordinal (ordered categories):\n",
        "* Education level (e.g., high school, bachelor's, master's, PhD)\n",
        "* Rating scale (e.g., 1 star, 2 stars, 3 stars, etc.)\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Values represent distinct categories.\n",
        "* May or may not have an inherent order (ordinal vs. nominal).\n",
        "* Represent qualitative data (descriptive).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "85f3ySfzkqb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.What is feature scaling? How does it help in Machine Learning?**"
      ],
      "metadata": {
        "id": "kGAYqyxI2Nj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Feature scaling is the process of adjusting the values of numerical features so they are on a similar scale. This helps machine learning models perform better and converge faster.\n",
        "\n",
        "**Types:**\n",
        "\n",
        "1.Normalization (Min-Max Scaling): Scales data to a range [0, 1].\n",
        "\n",
        "2.Standardization (Z-score Normalization): Scales data to have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "Why it's important:\n",
        "\n",
        "* Improves accuracy: Prevents features with larger values from dominating the model.\n",
        "* Faster convergence: Helps algorithms like gradient descent converge quicker.\n",
        "* Better results: Essential for distance-based algorithms (e.g., k-NN, SVM) and optimization-based algorithms (e.g., linear regression).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9yKDxVZw2NY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22.How do we perform scaling in Python?**"
      ],
      "metadata": {
        "id": "6XDVtLma2NN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:In Python, scikit-learn provides built-in tools to perform feature scaling. The most commonly used classes for scaling are MinMaxScaler for normalization and StandardScaler for standardization. Here's how you can use them:\n",
        "\n",
        "**1. Min-Max Scaling (Normalization)**\n",
        "\n",
        "To scale the features to a range [0, 1]:"
      ],
      "metadata": {
        "id": "UB4vyK762NDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv2R1x5c3I9m",
        "outputId": "1e51d0a4-cd5e-417e-f609-7b281ce16e43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Standardization (Z-score Normalization)**\n",
        "\n",
        "To scale the features to have a mean of 0 and a standard deviation of 1:"
      ],
      "metadata": {
        "id": "XipxD71B2M6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxKWH8ba3YKR",
        "outputId": "ce9d9e3e-5af6-4885-a30b-69e8ab29d0e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Steps:**\n",
        "\n",
        "* fit(): Computes the necessary parameters (mean, standard deviation, min, max, etc.) from the training data.\n",
        "\n",
        "* transform(): Applies the scaling transformation based on the computed parameters.\n",
        "\n",
        "* fit_transform(): A combination of fit() and transform(), used to fit the scaler and transform the data in one step.\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "* Min-Max Scaling: When you want your features to be in a specific range, typically [0, 1].\n",
        "\n",
        "* Standardization: When you need features to have zero mean and unit variance, typically when working with algorithms like logistic regression, SVM, and K-means.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1MzMW8dY3c1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "dV5sqt6z3tfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:sklearn.preprocessing is a module in scikit-learn that provides a variety of functions to preprocess and transform data before feeding it into machine learning models. It helps with tasks like scaling, encoding, handling missing values, and creating new features.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "**1.Scaling:**\n",
        "\n",
        "* MinMaxScaler: Scales features to a specific range (e.g., [0, 1]).\n",
        "* StandardScaler: Standardizes features to have a mean of 0 and standard deviation of 1.\n",
        "* RobustScaler: Scales features using median and interquartile range, less sensitive to outliers.\n",
        "\n",
        "**2.Encoding Categorical Variables:**\n",
        "\n",
        "* OneHotEncoder: Converts categorical variables into a one-hot (binary) format.\n",
        "* LabelEncoder: Converts labels into integers (useful for target variables).\n",
        "* OrdinalEncoder: Converts ordinal categorical variables (with a meaningful order) into integers.\n",
        "\n",
        "**3.Handling Missing Data:**\n",
        "\n",
        "* SimpleImputer: Imputes missing values with the mean, median, most frequent, or a constant value.\n",
        "* KNNImputer: Imputes missing values based on k-nearest neighbors.\n",
        "Binarization:\n",
        "\n",
        "**4.Binarizer:**\n",
        "* Converts numerical features into binary (0 or 1) based on a threshold.\n",
        "Polynomial Features:\n",
        "\n",
        "**5.PolynomialFeatures:**\n",
        "* Generates polynomial features from the original features (useful for nonlinear models).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "syBlikkj3tcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.How do we split data for model fitting (training and testing) in Python?**"
      ],
      "metadata": {
        "id": "rKiwUj3A3tYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:In Python, you can split your data into training and testing sets using train_test_split() from the scikit-learn library. This is essential to train the model on one portion of the data and test it on another to evaluate its performance."
      ],
      "metadata": {
        "id": "Bsz2JXzh3tWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example data (features X and target y)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([1, 0, 1, 0, 1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Test Features:\\n\", X_test)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Test Labels:\\n\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acS8db1t3ZdC",
        "outputId": "3aa67a01-2ce4-493f-e9c6-234c8de25cd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Test Features:\n",
            " [[3 4]]\n",
            "Training Labels:\n",
            " [1 1 1 0]\n",
            "Test Labels:\n",
            " [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters:**\n",
        "\n",
        "X: Input features (e.g., X for training data).\n",
        "\n",
        "y: Target labels (e.g., y for output values).\n",
        "\n",
        "test_size: Fraction of the dataset to include in the test split (e.g., 0.2 for 20% test data).\n",
        "\n",
        "random_state: Controls the shuffling of data before splitting. Setting it ensures reproducibility.\n",
        "\n",
        "**Output:**\n",
        "\n",
        "X_train, X_test: Training and testing feature sets.\n",
        "y_train, y_test: Training and testing labels.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GxpECALo56Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25.Explain data encoding?**"
      ],
      "metadata": {
        "id": "9bxi5ylB6BDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Data encoding is the process of converting categorical data (such as strings or labels) into a numerical format that can be used by machine learning models, which typically require numerical input. Many machine learning algorithms cannot work directly with text or non-numeric data, so encoding is an essential step in data preprocessing.\n",
        "\n",
        "There are several common techniques for encoding categorical data:\n",
        "\n",
        "**1. Label Encoding:**\n",
        "\n",
        "Definition: Label encoding converts each unique category into an integer. This is suitable when the categories have an inherent order or rank (ordinal data).\n",
        "\n",
        "Example:\n",
        "\n",
        "* Categories: [\"Low\", \"Medium\", \"High\"]\n",
        "* Encoded: [0, 1, 2]\n",
        "* When to Use: Use when the categorical variable has an ordinal relationship (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "\n",
        "Python Example:"
      ],
      "metadata": {
        "id": "H-J9igcu6BAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "categories = [\"Low\", \"Medium\", \"High\"]\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(categories)\n",
        "\n",
        "print(encoded_labels)  # Output: [0 1 2]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J590Fstq5yFi",
        "outputId": "712e9cac-9a3a-4858-e649-060379d89aa8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. One-Hot Encoding:**\n",
        "\n",
        "Definition: One-hot encoding creates a new binary (0 or 1) column for each category, representing whether the sample belongs to that category.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Categories: [\"Red\", \"Blue\", \"Green\"]\n",
        "* One-Hot Encoding\n",
        "\n",
        "* When to Use: Use for nominal categorical variables where no order exists between categories (e.g., colors, countries).\n",
        "\n",
        "\n",
        "**3. Ordinal Encoding:**\n",
        "\n",
        "Definition: Similar to label encoding but used specifically for ordinal data, where the categories have a meaningful order.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Categories: [\"Poor\", \"Fair\", \"Good\", \"Excellent\"]\n",
        "* Ordinal Encoding: [0, 1, 2, 3] where the numbers represent the order.\n",
        "* When to Use: Use for ordinal data where the categories have a natural ranking (e.g., rating scales).\n",
        "\n",
        "**4. Binary Encoding:**\n",
        "\n",
        "Definition: Binary encoding combines features of both label encoding and one-hot encoding. It converts categories into binary numbers and then splits each bit into a separate column.\n",
        "\n",
        "* When to Use: Use when dealing with high cardinality (many unique categories) where one-hot encoding would result in too many columns.\n",
        "\n",
        "**5. Target Encoding (Mean Encoding):**\n",
        "\n",
        "Definition: In target encoding, categorical values are replaced by the mean of the target variable for each category. This method is often used for high cardinality categorical variables.\n",
        "\n",
        "* When to Use: This is useful in cases where categorical variables have many levels, and one-hot encoding could result in a large number of columns.\n",
        "\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "* Label Encoding: Converts categories into numeric labels (good for ordinal data).\n",
        "* One-Hot Encoding: Converts categories into binary columns (good for nominal data).\n",
        "* Ordinal Encoding: Similar to label encoding, but specifically for ordinal categories.\n",
        "* Binary Encoding: A more compact encoding for high-cardinality data.\n",
        "* Target Encoding: Replaces categories with the mean of the target variable.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ll_UWaeJ6vv7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkIVngCp7AnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}